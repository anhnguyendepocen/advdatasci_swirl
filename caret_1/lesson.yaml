- Class: meta
  Course: advdatasci_swirl
  Lesson: caret_1
  Author: Detian Deng
  Type: Standard
  Organization: JHU Biostatistics and DSL
  Version: 2.4.2

- Class: text
  Output: In this swirl class, you will learn to use the caret package to create proper
    data split, and evaluate your predictive model performance.

- Class: text
  Output: First, let's review the steps of training, choosing, and evaluating a predictive
    (regression/classification) model.

- Class: text
  Output: '1. Divide you data set into 2 sets: training and testing, with ratio around
    70:30.'

- Class: text
  Output: 2. Further split training set into K (usually 10) parts, and use K-fold
    cross-validation to tune parameters or to choose between different models.

- Class: text
  Output: 3. Train the selected model with all data in training set and make prediction
    on the testing set, then calculate the out-of-sample error metric.

- Class: text
  Output: For regression problem, the error metric is usually Mean Square Error or
    Mean Absolute Error. For classification problem, the metric can be accuracy, kappa
    score, a weighted average of sensitivity and specificity or AUC of ROC curve.

- Class: text
  Output: In caret package, the function 'createDataPartition' can be used to create
    balanced splits of the data. If the y argument to this function is a factor, the
    random sampling occurs within each class and should preserve the overall class
    distribution of the data.

- Class: text
  Output: 'For example, to create a single 80/20% split of the iris data, you can
    use: createDataPartition(iris$Species, p = .8, list = FALSE, times = 1)'

- Class: cmd_question
  Output: Now try to creat a variable named trainIndex by the above function.
  AnswerTests: omnitest(correctExpr='trainIndex <- createDataPartition(iris$Species,
    p = .8, list = FALSE, times = 1)')
  CorrectAnswer: trainIndex <- createDataPartition(iris$Species, p = .8, list = FALSE,
    times = 1)
  Hint: trainIndex <- createDataPartition(iris$Species, p = .8, list = FALSE, times
    = 1)

- Class: text
  Output: The list = FALSE avoids returns the data as a list. This function also has
    an argument, times, that can create multiple splits at once; the data indices
    are returned in a list of integer vectors.

- Class: cmd_question
  Output: Then use this index variable to create your training set.
  AnswerTests: omnitest(correctExpr='irisTrain <- iris[trainIndex, ]')
  CorrectAnswer: irisTrain <- iris[trainIndex, ]
  Hint: irisTrain <- iris[trainIndex, ]

- Class: cmd_question
  Output: And your testing set as well
  AnswerTests: omnitest(correctExpr='irisTest  <- iris[-trainIndex, ]')
  CorrectAnswer: irisTest  <- iris[-trainIndex, ]
  Hint: irisTest  <- iris[-trainIndex, ]

- Class: text
  Output: 'To further create a 10-fold training set, you can use: createFolds(irisTrain$Species,
    k = 10, list = TRUE, returnTrain = FALSE)'

- Class: cmd_question
  Output: Now creat a variable named cvIndex by the above function.
  AnswerTests: omnitest(correctExpr='cvIndex <- createFolds(irisTrain$Species, k =
    10, list = TRUE, returnTrain = FALSE)')
  CorrectAnswer: cvIndex <- createFolds(irisTrain$Species, k = 10, list = TRUE, returnTrain
    = FALSE)
  Hint: cvIndex <- createFolds(irisTrain$Species, k = 10, list = TRUE, returnTrain
    = FALSE)

- Class: text
  Output: Although you can use the above index to program your own cross-validation,
    caret package offers a more convinient way to do it.

- Class: cmd_question
  Output: 'Use the following command to create an object to control your model training
    process: fitControl <- trainControl(method = "cv", number = 10)'
  AnswerTests: omnitest(correctExpr='fitControl <- trainControl(method = "cv", number
    = 10, repeats = 1, classProbs = TRUE, summaryFunction = twoClassSummary)')
  CorrectAnswer: fitControl <- trainControl(method = "cv", number = 10, repeats =
    1, classProbs = TRUE, summaryFunction = twoClassSummary)
  Hint: fitControl <- trainControl(method = "cv", number = 10, repeats = 1, classProbs
    = TRUE, summaryFunction = twoClassSummary)

- Class: text
  Output: Note that classProbs = TRUE, summaryFunction = twoClassSummary are specific
    to classification problems.

- Class: cmd_question
  Output: 'Then you can train your elastic net model using: glmnetFit <- train(Species
    ~ ., data = irisTrain, method = "glmnet", trControl = fitControl, metric = "ROC")'
  AnswerTests: omnitest(correctExpr='glmnetFit <- train(Species ~ ., data = irisTrain,
    method = "glmnet", trControl = fitControl, metric = "ROC")')
  CorrectAnswer: glmnetFit <- train(Species ~ ., data = irisTrain, method = "glmnet",
    trControl = fitControl, metric = "ROC")
  Hint: glmnetFit <- train(Species ~ ., data = irisTrain, method = "glmnet", trControl
    = fitControl, metric = "ROC")

- Class: text
  Output: Where you use metric = "ROC" to choose what type of error metric you want
    to use.

- Class: cmd_question
  Output: Now take a look at the cross-validation performance
  AnswerTests: omnitest(correctExpr='glmnetFit')
  CorrectAnswer: glmnetFit
  Hint: glmnetFit

- Class: text
  Output: It seams you have a perfect fitting regardless of the value of tuning parameters.
    If there were differences between the model performances, the train function would
    select the best model in terms of the metric you chose, train the model on the
    full training data and return it.

- Class: cmd_question
  Output: Therefore, you have already got yout best model in glmnetFit, now you can
    evaluate its performance on the testing data. First, make the predictions with
    the predict function and assign it to glmnetPred.
  AnswerTests: omnitest(correctExpr='glmnetPred <- predict(glmnetFit, newdata = irisTest)')
  CorrectAnswer: glmnetPred <- predict(glmnetFit, newdata = irisTest)
  Hint: glmnetPred <- predict(glmnetFit, newdata = irisTest)

- Class: cmd_question
  Output: Then you can evaluate the classification performance using the 'confusionMatrix'
    function.
  AnswerTests: omnitest(correctExpr='confusionMatrix(glmnetPred, irisTest$Species)')
  CorrectAnswer: confusionMatrix(glmnetPred, irisTest$Species)
  Hint: confusionMatrix(glmnetPred, irisTest$Species)

- Class: cmd_question
  Output: Now let's try regression. First, remove classProbs = TRUE, summaryFunction
    = twoClassSummary from the fitControl object.
  AnswerTests: omnitest(correctExpr='fitControl <- trainControl(method = "cv", number
    = 10, repeats = 1)')
  CorrectAnswer: fitControl <- trainControl(method = "cv", number = 10, repeats =
    1)
  Hint: fitControl <- trainControl(method = "cv", number = 10, repeats = 1)

- Class: cmd_question
  Output: 'Train regression model on one of its covariates: Petal.Width.'
  AnswerTests: omnitest(correctExpr='glmnetFit <- train(Petal.Width ~ ., data = irisTrain,
    method = "glmnet", trControl = fitControl)')
  CorrectAnswer: glmnetFit <- train(Petal.Width ~ ., data = irisTrain, method = "glmnet",
    trControl = fitControl)
  Hint: glmnetFit <- train(Petal.Width ~ ., data = irisTrain, method = "glmnet", trControl
    = fitControl)

- Class: cmd_question
  Output: Now take a look at the cross-validation performance
  AnswerTests: omnitest(correctExpr='glmnetFit')
  CorrectAnswer: glmnetFit
  Hint: glmnetFit

- Class: text
  Output: This time the model performances are different, and the best model based on the 
    default metric RMSE is returned.

- Class: cmd_question
  Output: Make predictions on the testing set and assign it to glmnetPred.
  AnswerTests: omnitest(correctExpr='glmnetPred <- predict(glmnetFit, newdata = irisTest)')
  CorrectAnswer: glmnetPred <- predict(glmnetFit, newdata = irisTest)
  Hint: glmnetPred <- predict(glmnetFit, newdata = irisTest)

- Class: cmd_question
  Output: Calculate the RMSE.
  AnswerTests: omnitest(correctExpr='sqrt(mean((glmnetPred - irisTest$Petal.Width)^2))')
  CorrectAnswer: sqrt(mean((glmnetPred - irisTest$Petal.Width)^2))
  Hint: sqrt(mean((glmnetPred - irisTest$Petal.Width)^2))

- Class: text
  Output: The out-of-sample RMSE is very close to the cross-validated RMSE. It suggests that the model is unlikely to be over-fitting.

- Class: text
  Output: Congratulations. You have completed this swirl class. For more information about caret package, you can go to 'http://topepo.github.io/caret/'.

- Class: mult_question
  Output: Would you like to submit the log of this lesson to Google Forms so that your instructor may evaluate your progress?
  AnswerChoices: Yes;No
  CorrectAnswer: Yes
  AnswerTests: submit_log()
  Hint: ""


